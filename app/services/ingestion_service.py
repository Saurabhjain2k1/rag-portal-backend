# app/services/ingest_langchain.py
import os
from sqlalchemy.orm import Session
from langchain_community.document_loaders import (
    PyPDFLoader,
    TextLoader,
    UnstructuredMarkdownLoader,
    UnstructuredWordDocumentLoader,
    UnstructuredCSVLoader,
    JSONLoader,
)
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma

from app.config import settings
from app.models.document import Document
from app.services.url_loader import load_url_with_bs4  # ðŸ‘ˆ our new function


def select_loader(path: str):
    ext = os.path.splitext(path)[1].lower()

    if ext == ".pdf":
        return PyPDFLoader(path)
    elif ext == ".txt":
        return TextLoader(path)
    elif ext == ".md":
        return UnstructuredMarkdownLoader(path)
    elif ext == ".docx":
        return UnstructuredWordDocumentLoader(path)
    elif ext == ".csv":
        return UnstructuredCSVLoader(path)
    elif ext == ".json":
        return JSONLoader(path, jq_schema=".", text_content=False)
    else:
        raise ValueError(f"Unsupported file type: {ext}")


def ingest_with_langchain(db: Session, document: Document, tenant_id: int) -> int:
    """
    Full LangChain ingestion using:
      - Local HuggingFace embeddings
      - Chroma as vector store
      - Files (pdf/txt/md/docx/csv/json) OR URLs
    """

    path = document.storage_path

    # 1) Load docs: URL vs file
    if path.startswith("http://") or path.startswith("https://"):
        docs = load_url_with_bs4(path)
    else:
        loader = select_loader(path)
        docs = loader.load()

    # 2) Chunk
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        separators=["\n\n", "\n", ".", " "],
    )
    chunks = splitter.split_documents(docs)

    for c in chunks:
        c.metadata["tenant_id"] = tenant_id
        c.metadata["document_id"] = document.id

    # 3) Local embeddings
    embeddings = HuggingFaceEmbeddings(model_name=settings.local_embed_model)

    # 4) Chroma vector store
    collection_name = f"tenant_{tenant_id}"
    persist_dir = settings.chroma_dir

    Chroma.from_documents(
        documents=chunks,
        embedding=embeddings,
        collection_name=collection_name,
        persist_directory=persist_dir,
    )

    return len(chunks)
